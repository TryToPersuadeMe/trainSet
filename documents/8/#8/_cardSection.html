<section class="projects">
  <div class="container">
    <h2 class="title projects__title">TrainSet Academy</h2>

    <div class="cardSection">
      <div class="navigationSlider">
        <div class="navigationSlider__arrow swiper-button-prev"></div>

        <div class="navigationSlider__container swiper-container">
          <div class="navigationSlider__wrapper swiper-wrapper">
            <a href="#" class="navigationSlider__link swiper-slide active">Doks</a>
            <a href="#" class="navigationSlider__link swiper-slide">Blog</a>
            <a href="#" class="navigationSlider__link swiper-slide">Training</a>
          </div>
        </div>

        <div class="navigationSlider__arrow swiper-button-next"></div>
      </div>

      @@include("./_navUlListContetnt.html")

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">1. Introduction</h6>

        <p class="paragraph__text">There are two types of decision trees:</p>

        <ul class="paragraph__text paragraph__list paragraph__list_dots-type">
          <li>
            Classification tree - used with categorical data: the predicted outcome is the class to which the data belongs. For
            example, an outcome of a loan application as ‘safe’ or ‘risky’.
          </li>

          <li>
            Regression tree - used with continuous data: the predicted outcome is a real number. For example, a population of a
            state or inhabitant height in centimeters.
          </li>
        </ul>

        <p class="paragraph__text">
          Thus, decision trees can handle both categorical and numerical data. This section conveys decision trees for
          classification problems. For Regression trees, go
          <a href="https://ml-book.com/docs/cl_rf/" class="paragraph__text_link">here</a>.
        </p>
      </div>
      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">2. Decision Trees in Classification</h6>

        <p class="paragraph__text">
          Decision tree builds classification models in the form of a tree structure. It breaks down a dataset into smaller and
          smaller subsets by learning a series of explicit if-then rules on feature values that results in predicting a target
          value.
        </p>

        <p class="paragraph__text">
          A decision tree consists of the decision nodes and leaf nodes. A decision node (Outlook or Wind) has two or more
          branches (e.g., Sunny, Overcast and Rain). <span class="paragraph__text_bold">Leaf node</span> (e.g., Play Golf)
          represents a classification (i.e. decision), and it is an endpoint (last node) of any branch (Yes, No, No, Yes). The
          topmost decision node in a tree which corresponds to the best predictor called
          <span class="paragraph__text_bold">root node</span> (Outlook).
        </p>

        <div class="nestedPicture cardSection__nestedPicture">
          <div class="nestedPicture__image"><img src="./resources/images/academy/1.png" alt="#" /></div>
        </div>

        <!-- for margin beetween text and image -->
        <p class="paragraph__text"></p>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">3. ID3 Algorithm</h6>

        <p class="paragraph__text">
          There are various decision tree algorithms, namely, ID3 (Iterative Dichotomiser 3), C4.5 (successor of ID3), CART
          (Classification and Regression Tree), CHAID (Chi-square Automatic Interaction Detector), MARS. This article is about a
          classification decision tree with ID3 algorithm.
        </p>

        <p class="paragraph__text">
          One of the core algorithms for building decision trees is ID3 by
          <span class="paragraph__text_blue">J. R. Quinlan</span>. ID3 is used to generate a decision tree from a dataset commonly
          represented by a table. To construct a decision tree, ID3 uses a top-down, greedy search through the given columns,
          where each column (further called <span class="paragraph__text_bold">attribute</span>) at every tree node is tested, and
          selects the attribute that is best for classification of a given set. To decide what attribute is best to select to
          construct a decision tree, ID3 uses <span class="paragraph__text_bold">Entropy</span> and
          <span class="paragraph__text_bold">Information Gain</span>.
        </p>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">4. Entropy & Information Gain</h6>

        <p class="paragraph__text paragraph__text_big paragraph__text_bold">Entropy (E)</p>
        <p class="paragraph__text">
          Entropy is the measure of the <span class="paragraph__text_bold">amount of uncertainty</span> or
          <span class="paragraph__text_bold">randomness</span> in data. Intuitively, it shows predictability of a certain event.
          If an outcome of an event has a probability of 100%, the entropy is zero (no randomness exists), and if an outcome is
          50%, the entropy takes the maximum value (i.e. equals to 1 since it is the
          <span class="paragraph__text_blue">log base 2</span>) as it projects perfect randomness. For example, consider a coin
          toss whose probability of heads is 0.5 and probability of tails is 0.5. The entropy here is the highest possible value
          (i.e., equals 1), since there’s no chance to precisely determine the outcome. Alternatively, consider a coin which has
          heads on both the sides, the outcome of such an event can be predicted perfectly since we know beforehand that it will
          always be heads. In other words, this event has no randomness, hence its entropy is zero.
          <span class="paragraph__text_bold">
            ID3 follows the rule: a branch with an entropy of 0 is a leaf node (endpoint). A branch with an entropy more than 0
            needs further splitting.</span
          >
          In case it is not possible to achieve zero entropy in the leaf nodes, the decision is made by the method of a
          <span class="paragraph__text_bold">simple majority</span>.
        </p>

        <p class="paragraph__text">
          To build a decision tree, we need to calculate two types of entropy using frequency tables as follows:
        </p>

        <p class="paragraph__text">
          1.Entropy E ( S ) using the frequency table of one attribute, where S is a current state (existing outcomes) and P ( x )
          is a probability of an event x of that state S :
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>x</mi>
                <mi>e</mi>
                <mi>p</mi>
                <mi>s</mi>
                <mi>i</mi>
                <mi>l</mi>
                <mi>o</mi>
                <mi>n</mi>
                <mi>X</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD"> </mrow>
            </munderover>
            <mo>&#x2212;<!-- − --></mo>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
          </math>
        </div>

        <p class="paragraph__text">
          1.Entropy E ( S , A ) using the frequency table of two attributes - S and A , where S is a current state with an
          attribute A (existing outcomes with an attribute A), A is a selected attribute, and P(x) is a probability of an event x
          of an attribute A.
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>A</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>x</mi>
                <mi>e</mi>
                <mi>p</mi>
                <mi>s</mi>
                <mi>i</mi>
                <mi>l</mi>
                <mi>o</mi>
                <mi>n</mi>
                <mi>X</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD"> </mrow>
            </munderover>
            <mo stretchy="false">[</mo>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo stretchy="false">(</mo>
            <mn>2</mn>
            <mo stretchy="false">)</mo>
            <mo stretchy="false">]</mo>
          </math>
        </div>

        <p class="paragraph__text">
          E ( S ) is the Entropy of the entire set, while the second term E ( S , A ) relates to an Entropy of an attribute A.
        </p>

        <p class="paragraph__text paragraph__text_big paragraph__text_bold">Information Gain (IG)</p>

        <p class="paragraph__text">
          Information gain (also called as Kullback-Leibler divergence) denoted by I G ( S , A ) for a state S is the
          <span class="paragraph__text_bold"> change in entropy</span> after deciding on a particular attribute A . It measures
          the relative change (decrease) in entropy with respect to the independent variables, as follows:
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>A</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>A</mi>
            <mo stretchy="false">)</mo>
          </math>
        </div>

        <p class="paragraph__text">
          The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a
          decision tree is all about selecting each attribute ( A ) to calculate Information Gain and finding such an attribute
          that returns the highest IG (i.e., the most homogeneous branches). This attribute will be the next decision node for the
          tree.
        </p>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">5. Example</h6>

        <p class="paragraph__text">Let’s understand this with the help of an example outlined in the beginning.</p>
        <p class="paragraph__text">
          Consider a piece of data collected over the course of 14 days where the features are Outlook, Temperature, Humidity,
          Wind and the outcome variable is whether Golf was played on the day. Now, our job is to build a predictive model which
          takes in above 4 parameters and predicts whether Golf will be played on the day. We’ll build a decision tree to do that
          using ID3 algorithm.
        </p>

        <div class="nestedPicture cardSection__nestedPicture">
          <div class="nestedPicture__image"><img src="./resources/images/academy/2.png" alt="#" /></div>
        </div>

        <p class="paragraph__text">ID3 Algorithm will perform following tasks recursively:</p>

        <ol class="paragraph__text paragraph__list">
          <li>Create a root node for the tree</li>
          <li>If all examples are positive, return leaf node ‘positive’</li>
          <li>Else if all examples are negative, return leaf node ‘negative’</li>
          <li>Calculate the entropy of current state E ( S )</li>
          <li>For each attribute, calculate the entropy with respect to the attribute ‘A ’ denoted by E ( S , A )</li>
          <li>
            Select the attribute which has the maximum value of I G ( S , A ) and split the current (parent) node on the selected
            attribute
          </li>
          <li>Remove the attribute that offers highest I G from the set of attributes</li>
          <li>Repeat until we run out of all attributes, or the decision tree has all leaf nodes.</li>
        </ol>

        <p class="paragraph__text">
          Now we’ll go ahead and grow the decision tree. The initial step is to calculate E ( S ) , the Entropy of the current
          state (i.e. existing outcomes at this stage). In the above example, we can see in total there are 9 Yes’s and 5 No’s.
        </p>

        <div class="nestedPicture cardSection__nestedPicture">
          <div class="nestedPicture__image"><img src="./resources/images/academy/3.png" alt="#" /></div>
        </div>

        <p class="paragraph__text">Let’s calculate E ( S ) using the formula (1):</p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>6</mn>
              <mn>8</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>6</mn>
              <mn>8</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>2</mn>
              <mn>8</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>2</mn>
              <mn>8</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.811</mn>
          </math>
        </div>

        <p class="paragraph__text">
          Remember that the Entropy is 0 if all members belong to the same class, and 1 when half of them belong to one class and
          other half belong to other class, which is perfect randomness. Here it’s 0.94, which means the distribution is
          <span class="paragraph__text_bold"> fairly random. </span>
        </p>

        <p class="paragraph__text">
          Now the next step is to choose the attribute that gives us highest possible Information Gain which we’ll choose as the
          root node. Let’s start with ‘Wind’ attribute, calculating its E ( S , W i n d ) and I G ( S , W i n d ) :
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>W</mi>
            <mi>i</mi>
            <mi>n</mi>
            <mi>d</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>W</mi>
            <mi>i</mi>
            <mi>n</mi>
            <mi>d</mi>
            <mo stretchy="false">)</mo>
            <mo stretchy="false">&#x2192;<!-- → --></mo>
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>W</mi>
            <mi>I</mi>
            <mi>n</mi>
            <mi>d</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>x</mi>
                <mi>e</mi>
                <mi>p</mi>
                <mi>s</mi>
                <mi>i</mi>
                <mi>l</mi>
                <mi>o</mi>
                <mi>n</mi>
                <mi>X</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD"> </mrow>
            </munderover>
            <mo stretchy="false">[</mo>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo stretchy="false">]</mo>
          </math>
        </div>

        <p class="paragraph__text">
          where ‘x’ in P ( x ) are the possible values for an attribute. Here, attribute ‘Wind’ takes two possible values in the
          sample data.
        </p>

        <p class="paragraph__text">Hence, x = W e a k , S t r o n g</p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>W</mi>
            <mi>i</mi>
            <mi>n</mi>
            <mi>d</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>t</mi>
            <mi>r</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>g</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>t</mi>
            <mi>r</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>g</mi>
            <mo stretchy="false">)</mo>
          </math>
        </div>

        <p class="paragraph__text">Thus, we have to find the following terms:</p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mspace linebreak="newline" />
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>t</mi>
            <mi>r</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>g</mi>
            <mo stretchy="false">)</mo>
            <mspace linebreak="newline" />
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mspace linebreak="newline" />
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>t</mi>
            <mi>r</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>g</mi>
            <mo stretchy="false">)</mo>
            <mspace linebreak="newline" />
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.940</mn>
          </math>
        </div>

        <p class="paragraph__text">, which we have already calculated</p>
        <p class="paragraph__text">
          Amongst all the 14 examples we have 8 places where the wind is Weak and 6 where the wind is
          <span class="paragraph__text_blue">Strong</span>.
        </p>

        <div class="nestedPicture cardSection__nestedPicture">
          <div class="nestedPicture__image"><img src="./resources/images/academy/4.png" alt="#" /></div>
        </div>

        <p class="paragraph__text">
          Now out of the 8 Weak examples, 6 of them were ‘Yes’ for Play Golf and 2 of them were ‘No’ for ‘Play Golf’. So, let’s
          calculate an entropy for <span class="paragraph__text_blue">“Weak”</span> values of
          <span class="paragraph__text_blue">Wind</span> attribute:
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>6</mn>
              <mn>8</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>6</mn>
              <mn>8</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>2</mn>
              <mn>8</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>2</mn>
              <mn>8</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.811</mn>
          </math>
        </div>

        <ul class="paragraph__text paragraph__list">
          <li class="nestedPicture paragraph__list_nestedPicture">
            <span>
              Similarly, out of 6 Strong examples, we have 3 examples where the outcome was ‘Yes’ for Play Golf and 3 where we had
              ‘No’ for Play Golf.</span
            >

            <div class="nestedPicture__image"><img src="./resources/images/academy/5.png" alt="#" /></div>
          </li>
        </ul>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>t</mi>
            <mi>r</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>g</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>3</mn>
              <mn>6</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>3</mn>
              <mn>6</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>3</mn>
              <mn>6</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>3</mn>
              <mn>6</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>1.000</mn>
          </math>
        </div>

        <p class="paragraph__text">
          Remember, here half items belong to one class while other half belong to other. Hence we have perfect randomness.
        </p>
        <p class="paragraph__text">Now we have all the pieces required to calculate the Information Gain:</p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>W</mi>
            <mi>i</mi>
            <mi>n</mi>
            <mi>d</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>w</mi>
            </msub>
            <mi>e</mi>
            <mi>a</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mi>P</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>t</mi>
            <mi>r</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>g</mi>
            <mo stretchy="false">)</mo>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>t</mi>
            <mi>r</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>g</mi>
            <mo stretchy="false">)</mo>
            <mspace linebreak="newline" />
            <mo>=</mo>
            <mn>0.940</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>8</mn>
              <mn>14</mn>
            </mfrac>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mn>0.811</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>6</mn>
              <mn>14</mn>
            </mfrac>
            <mo>&#x2217;<!-- ∗ --></mo>
            <mn>1</mn>
            <mspace linebreak="newline" />
            <mo>=</mo>
            <mn>0.048</mn>
          </math>
        </div>

        <p class="paragraph__text">
          That tells us the Information Gain by considering ‘Wind’ as the attribute and gives us information gain of 0.048. Now
          the next step is to choose the attribute that gives us highest possible Information Gain which we’ll choose as the root
          node. Therefore, we must similarly calculate the Information Gain for all the other attributes and pick the one with the
          highest score.
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>O</mi>
            <mi>u</mi>
            <mi>t</mi>
            <mi>l</mi>
            <mi>o</mi>
            <mi>o</mi>
            <mi>k</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.246</mn>
            <mspace linebreak="newline" />
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>T</mi>
            <mi>e</mi>
            <mi>m</mi>
            <mi>p</mi>
            <mi>e</mi>
            <mi>r</mi>
            <mi>a</mi>
            <mi>t</mi>
            <mi>u</mi>
            <mi>r</mi>
            <mi>e</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.029</mn>
            <mspace linebreak="newline" />
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>H</mi>
            <mi>u</mi>
            <mi>m</mi>
            <mi>i</mi>
            <mi>d</mi>
            <mi>i</mi>
            <mi>t</mi>
            <mi>y</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.151</mn>
            <mspace linebreak="newline" />
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <mi>S</mi>
            <mo>,</mo>
            <mi>W</mi>
            <mi>i</mi>
            <mi>n</mi>
            <mi>d</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.048</mn>
          </math>
        </div>

        <p class="paragraph__text">(calculated in a previous example)</p>

        <p class="paragraph__text">
          We can clearly see that IG(S, Outlook) has the highest information gain of
          <span class="paragraph__text_bold">0.246</span>, hence we chose
          <span class="paragraph__text_bold">Outlook</span> attribute as the root node. At this point, the decision tree looks
          like:
        </p>

        <div class="nestedPicture cardSection__nestedPicture">
          <div><img src="./resources/images/academy/6.png" alt="#" /></div>
        </div>

        <p class="paragraph__text">
          Here we observe that whenever the outlook at <span class="paragraph__text_blue">Overcast, Play Golf</span> is always
          ‘Yes’. That means, the entropy is 0 and we can leave “Yes” as a leaf node. The fact that Overcast is always yes is not a
          coincidence by any chance, the simple tree resulted due to the highest information gain, given by the attribute Outlook.
        </p>

        <p class="paragraph__text">
          Now how do we proceed from this point? We can simply apply recursion: you might want to look at the algorithm steps
          described earlier.
        </p>

        <p class="paragraph__text">
          Now that we have used Outlook, we have got three of them remaining:
          <span class="paragraph__text_blue">Humidity, Temperature, </span> and <span class="paragraph__text_blue">Wind</span>.
          And, we had three possible values of Outlook: Sunny, Overcast, Rain. Where the Overcast node already ended up having
          leaf node ‘Yes’, so we’re left with two subtrees to compute: <span class="paragraph__text_blue">Sunny</span> and
          <span class="paragraph__text_blue">Rain</span>. Let’s start with <span class="paragraph__text_blue">Sunny</span>, and
          compute its entropy.
        </p>
        <p class="paragraph__text">
          Amongst all the 5 examples the attribute value of Outlook is Sunny, 2 of them were ‘Yes’ for Play Golf and 3 of them
          were ‘No’ for ‘Play Golf’.
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>E</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>u</mi>
            <mi>n</mi>
            <mi>n</mi>
            <mi>y</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>3</mn>
              <mn>5</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>3</mn>
              <mn>5</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>&#x2212;<!-- − --></mo>
            <mfrac>
              <mn>2</mn>
              <mn>5</mn>
            </mfrac>
            <mi>l</mi>
            <mi>o</mi>
            <msub>
              <mi>g</mi>
              <mn>2</mn>
            </msub>
            <mo stretchy="false">(</mo>
            <mfrac>
              <mn>2</mn>
              <mn>5</mn>
            </mfrac>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.96</mn>
          </math>
        </div>

        <p class="paragraph__text">In the similar fashion, we compute the following values:</p>

        <div class="paragraph__text paragraph__text_math-code">
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>u</mi>
            <mi>n</mi>
            <mi>n</mi>
            <mi>y</mi>
            <mo>,</mo>
            <mi>H</mi>
            <mi>u</mi>
            <mi>m</mi>
            <mi>i</mi>
            <mi>d</mi>
            <mi>i</mi>
            <mi>t</mi>
            <mi>y</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.963</mn>
            <mspace linebreak="newline" />
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>u</mi>
            <mi>n</mi>
            <mi>n</mi>
            <mi>y</mi>
            <mo>,</mo>
            <mi>T</mi>
            <mi>e</mi>
            <mi>m</mi>
            <mi>p</mi>
            <mi>e</mi>
            <mi>r</mi>
            <mi>a</mi>
            <mi>t</mi>
            <mi>u</mi>
            <mi>r</mi>
            <mi>e</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.570</mn>
            <mspace linebreak="newline" />
            <mi>I</mi>
            <mi>G</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>S</mi>
              <mi>s</mi>
            </msub>
            <mi>u</mi>
            <mi>n</mi>
            <mi>n</mi>
            <mi>y</mi>
            <mo>,</mo>
            <mi>W</mi>
            <mi>i</mi>
            <mi>n</mi>
            <mi>d</mi>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>0.019</mn>
            <mspace linebreak="newline" />
          </math>
        </div>

        <p class="paragraph__text">
          As we can see the highest Information Gain is given by <span class="paragraph__text_bold">Humidity</span>. Proceeding in
          the same way with S r a i n will give us <span class="paragraph__text_blue">Wind</span> as the one with
          <span class="paragraph__text_blue">highest information gain</span>.
        </p>

        <p class="paragraph__text">The final Decision Tree is going to be looked as such:</p>

        <div class="nestedPicture cardSection__nestedPicture">
          <div><img src="./resources/images/academy/6.png" alt="#" /></div>
        </div>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">6. Summary</h6>

        <p class="paragraph__text">
          A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain
          instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the
          sample is completely homogeneous, the entropy is zero and if the sample is an equally divided it has an entropy of one.
        </p>

        <p class="paragraph__text">
          The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a
          decision tree is all about finding an attribute that returns the highest information gain (i.e. the most homogeneous
          branches, or the lowest entropy). After that, all the outcome instances that are possible are examined whether they
          belong to the same class or not. For the instances of the same class, a single name class is used to denote otherwise
          the instances are classified on the basis of splitting attribute.
        </p>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">7. Overfitting and Pruning</h6>

        <p class="paragraph__text">
          One of the most common problems with decision trees, especially the ones that have a table full of columns, is that they
          tend to <span class="paragraph__text_bold">overfit</span> a lot. Sometimes it looks like the tree just
          <span class="paragraph__text_blue">memorizes</span> the data. Here are the typical examples of decision trees that
          overfit, both for categorical and continuous data:
        </p>

        <p class="paragraph__text">
          Categorical:<span class="paragraph__text_blue"
            >If the client is male, between 15 and 25, from the US, likes ice-cream, has a German friend, hates birds and ate
            pancakes on August 25th, 2012, - he is likely to download Pokemon Go.</span
          >
        </p>

        <ul class="paragraph__text paragraph__list">
          <li class="nestedPicture paragraph__list_nestedPicture">
            <span> Continuous: </span>

            <div class="nestedPicture__image"><img src="./resources/images/academy/5.png" alt="#" /></div>
          </li>
        </ul>

        <p class="paragraph__text">There are two main ways to mitigate overfitting in Decision Trees:</p>

        <ol class="paragraph__text paragraph__list">
          <li>Using Random Forests</li>
          <li>Pruning Decision Trees</li>
        </ol>

        <p class="paragraph__text">
          Random Forest prevents the problem of overfitting as it is an ensemble of (<span class="paragraph__text_blue">n</span>)
          decision trees, not just one, with <span class="paragraph__text_blue">n</span> results in the end. The final result of
          Random forest is the most frequent response variable (the mode) among n results (of
          <span class="paragraph__text_blue">n</span> Decision Trees). We will not explain this algorithm in this section. It is a
          separate algorithm and you can read the article on it
          <a class="paragraph__text_link" href="https://ml-book.com/docs/cl_rf/">here</a> in detail.
        </p>

        <p class="paragraph__text">
          Pruning involves the removal of nodes and branches in a decision tree to make it simpler so as to mitigate overfitting
          and improve performance. Ideally, we want the leaf nodes to be as little randomized as possible for high accuracy, but
          it is very easy to overfit, so much so, that in many cases, the leaf nodes may only have a single data point. We can
          mitigate this by pruning the decision tree by a method called
          <span class="paragraph__text_bold">cost-effective pruning</span>.
        </p>

        <p class="paragraph__text">The following algorithm takes place while applying cost-effective pruning:</p>
        <p class="paragraph__text">Determine the performance of the original tree, T, with the validation data</p>
        <p class="paragraph__text">
          Consider a sub-tree, t(1), and remove it from the original tree, replacing a sub-tree with a leaf.
        </p>
        <p class="paragraph__text">Determine the performance of a new tree, T(new).</p>

        <p class="paragraph__text">
          If the delta in performance is insignificant (that is, if validation set does not have the significant difference in
          delta performance), consider simpler (pruned) tree (Occam’s razor) as an original, and continue to the next sub-tree.
        </p>

        <p class="paragraph__text">number of leaves</p>
        <p class="paragraph__text">Original tree T</p>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">8. Pros & Cons​</h6>

        <p class="paragraph__text paragraph__text_big paragraph__text_bold">Advantages of ID3</p>

        <ol class="paragraph__text paragraph__list_bold-marker paragraph__list">
          <li>Easily visualized and interpreted. The training data is used to create understandable prediction rules.</li>
          <li>No feature normalization is typically needed.</li>
          <li>The calculation time of ID3 is the linear function of the product of the characteristic number and node number.</li>
          <li>Works well with datasets using a mixture of feature types (continuous/categorical/binary)</li>
        </ol>

        <p class="paragraph__text paragraph__text_big paragraph__text_bold">Disadvantages of ID3</p>

        <ol class="paragraph__text paragraph__list_bold-marker paragraph__list">
          <li>
            Data may be <span class="paragraph__text_bold">overfitted</span> or
            <span class="paragraph__text_bold">overclassified</span>.
          </li>
          <li>No feature normalization is typically needed.</li>
          <li>For making a decision, only one attribute is tested at an instant thus consuming a lot of time.</li>
          <li>
            Classifying the continuous data may prove to be expensive in terms of computation, as many trees have to be generated
            to see where to break the continuum. One disadvantage of ID3 is that when given a large number of input values, it is
            overly sensitive to features with a large number of values.
          </li>
        </ol>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">9. Decision Tree in Python</h6>

        <p class="paragraph__text">View/download a template of Decision Tree located in a git repository here .</p>
      </div>
    </div>

    @@include("./_fixedArrow.html")
  </div>
</section>
