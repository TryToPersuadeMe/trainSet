<section class="projects">
  <div class="container">
    <h2 class="title projects__title">TrainSet Academy</h2>

    <div class="cardSection">
      <div class="navigationSlider">
        <div class="navigationSlider__arrow swiper-button-prev"></div>

        <div class="navigationSlider__container swiper-container">
          <div class="navigationSlider__wrapper swiper-wrapper">
            <a href="#" class="navigationSlider__link swiper-slide active">Doks</a>
            <a href="#" class="navigationSlider__link swiper-slide">Blog</a>
            <a href="#" class="navigationSlider__link swiper-slide">Training</a>
          </div>
        </div>

        <div class="navigationSlider__arrow swiper-button-next"></div>
      </div>

      @@include("./_navUlListContetnt.html")

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">1. Introduction to Lasso Regularization Term (L1)</h6>

        <p class="paragraph__text">
          LASSO - Least Absolute Shrinkage and Selection Operator - was first formulated by Robert Tibshirani in 1996. It is a
          powerful method that performs two main tasks: regularization and feature selection.
        </p>

        <p class="paragraph__text">
          Let’s look at the example of lasso regularization with linear models, where OLS method is used with its regularization
          term.
        </p>

        <div class="nestedPicture cardSection__nestedPicture nestedPicture_math-code">
          <div class="nestedPicture__image"><img src="./resources/images/academy/1.png" alt="#" /></div>
        </div>

        <p class="paragraph__text">
          The LASSO method puts a constraint on the sum of the absolute values of the model parameters, the sum has to be less
          than a fixed value (upper bound, or ):
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math class="paragraph__text paragraph__text_math-code" xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>k</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mrow>
              <mo>|</mo>
              <msub>
                <mi>&#x03B2;<!-- β --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo>|</mo>
            </mrow>
            <mo>&lt;</mo>
            <mi>t</mi>
            <mo>,</mo>
          </math>
        </div>

        <p class="paragraph__text">–where t is the upper bound for the sum of the coefficients.</p>

        <p class="paragraph__text">
          In order to do so, the method applies a shrinking (regularization) process where it penalizes the coefficients of the
          regression variables shrinking some of them to zero. During features selection process the variables that still have a
          non-zero coefficient after the shrinking process are selected to be part of the model. The goal of this process is to
          minimize the prediction error.
        </p>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">2. Parameter alpha ()</h6>

        <div class="paragraph__text paragraph__text_math-code">
          <math class="paragraph__text paragraph__text_math-code" xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>&#x03B1;<!-- α --></mi>
          </math>
        </div>

        <p class="paragraph__text">
          In practice, the tuning parameter α that controls the strength of the penalty assumes great importance. Indeed, when α
          is sufficiently large, coefficients are forced to be exactly equal to zero. This way, dimensionality can be reduced. The
          larger the parameter α, the more the number of coefficients are shrunk to zero. On the other hand, if α = 0, we have
          just an OLS (Ordinary Least Squares) regression.
        </p>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">3. Advanages</h6>

        <p class="paragraph__text">There are many advantages of using the LASSO method.</p>

        <ul class="paragraph__list paragraph__text paragraph__list_dots-type">
          <li>
            First of all, it can provide a very good prediction accuracy, because shrinking and removing the coefficients can
            reduce variance without a substantial increase of the bias, this is especially useful when you have a small number of
            observation and a large number of features. In terms of the tuning parameter α we know that bias increases and
            variance decreases when α increases, indeed a trade-off between bias and variance has to be found.
          </li>
          <li>
            Moreover, the LASSO helps to increase the model interpretability by eliminating irrelevant variables that are not
            associated with the response variable, this way also overfitting is reduced. This is the point where we are more
            interested in because in this paper the focus is on the feature selection task.
          </li>
        </ul>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">4. Introduction to Lasso Regression</h6>

        <p class="paragraph__text">
          Lasso with linear models is called Lasso Regression. It is the model that describes the relationship between response
          variable Y and explanatory variables X. In the case of one explanatory variable, Lasso Regression is called Simple Lasso
          Regression while the case with two or more explanatory variables is called Multiple Lasso Regression.
        </p>

        <p class="paragraph__text">Lasso Regression holds all the assumptions of the Linear Regression, such as:</p>

        <ul class="paragraph__list paragraph__text paragraph__list_dots-type">
          <li>The response variable is normally distributed;</li>
          <li>There is a linear relationship between the response variable and the explanatory variables;</li>
          <li>
            The random errors are normally distributed, have constant (equal) variances at any point in X, and are independent.
          </li>
        </ul>

        <div>
          <p class="paragraph__text">
            <span class="paragraph__text_italic">To read more about Linear Regression assumptions</span>, go to
            <a href="https://www.thelearningmachine.ai/cnn" class="paragraph__text_link">Linear Regression</a>
          </p>
        </div>
      </div>

      <div class="paragraph cardSection__paragraph">
        <h6 class="paragraph__title paragraph__title_BG">5. The Model</h6>

        <p class="paragraph__text">
          The LASSO minimizes the sum of squared errors, with an upper bound on the sum of the absolute values of the model
          parameters. The lasso estimate is defined by the solution to the L1 optimization problem:
        </p>

        <div class="paragraph__text paragraph__text_math-code">
          <math class="paragraph__text paragraph__text_math-code" xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mo>&lt;</mo>
            <mi>i</mi>
            <mo>&gt;</mo>
            <mi>m</mi>
            <mi>i</mi>
            <mi>n</mi>
            <mi>i</mi>
            <mi>m</mi>
            <mi>i</mi>
            <mi>z</mi>
            <mi>e</mi>
            <mo>&lt;</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>/</mo>
            </mrow>
            <mi>i</mi>
            <mo>&gt;</mo>
            <mrow>
              <mo>[</mo>
              <mfrac>
                <mrow>
                  <munderover>
                    <mo>&#x2211;<!-- ∑ --></mo>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x0131;<!-- ı --></mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                    </mrow>
                  </munderover>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>y</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>i</mi>
                    </mrow>
                  </msub>
                  <mo>&#x2212;<!-- − --></mo>
                  <mo stretchy="false">)</mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>&#x03B2;<!-- β --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>i</mi>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>i</mi>
                    </mrow>
                  </msub>
                  <mo>+</mo>
                  <msub>
                    <mi>&#x03B2;<!-- β --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>0</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <msup>
                    <mo stretchy="false">)</mo>
                    <mn>2</mn>
                  </msup>
                </mrow>
                <mi>n</mi>
              </mfrac>
              <mo>]</mo>
            </mrow>
            <mo>&lt;</mo>
            <mi>i</mi>
            <mo>&gt;</mo>
            <mi>s</mi>
            <mi>u</mi>
            <mi>b</mi>
            <mi>j</mi>
            <mi>e</mi>
            <mi>c</mi>
            <mi>t</mi>
            <mi>t</mi>
            <mi>o</mi>
            <mo>&lt;</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>/</mo>
            </mrow>
            <mi>i</mi>
            <mo>&gt;</mo>
            <mrow>
              <mo>[</mo>
              <mrow>
                <mi>&#x03B1;<!-- α --></mi>
                <munderover>
                  <mo>&#x2211;<!-- ∑ --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>&#x0237;<!-- ȷ --></mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>k</mi>
                  </mrow>
                </munderover>
                <mrow>
                  <mo>|</mo>
                  <msub>
                    <mi>&#x03B2;<!-- β --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>&#x0237;<!-- ȷ --></mi>
                    </mrow>
                  </msub>
                  <mo>|</mo>
                </mrow>
                <mo>&lt;</mo>
                <mi>t</mi>
              </mrow>
              <mo>]</mo>
            </mrow>
            <mo>,</mo>
          </math>
        </div>
      </div>
    </div>

    @@include("./_fixedArrow.html")
  </div>
</section>
